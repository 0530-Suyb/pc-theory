[toc]

## 基础性能测试

### 实验环境

| 硬件 | 性能                                                   |
| ---- | ------------------------------------------------------ |
| CPU  | Inter i7-10700 8核16线程 基本频率2.9Ghz 最大睿频4.8Ghz |
| 内存 | 16GiB                                                  |
| 硬盘 | 512GB                                                  |

| 软件           | 版本    |
| -------------- | ------- |
| ubuntu         | 20.04   |
| caliper        | 0.5     |
| docker         | 24.0.5  |
| docker-compose | 1.25.0  |
| Fabric         | v2.2.14 |
| PC-Fabric      |         |
| PC-P3Chain     |         |



### 实验设置

#### PC-Fabric

对照组设置：**Fabric** vs **PC-Fabric one pool** vs **PC-Fabric three pools**

- 单池单合约：Fabric vs PC-Fabric（原Fabric相当于单池）
- PC-Fabric自对比：单池单合约 vs 3池3合约（各池各1个合约，各个合约的具体函数都是set和get两种，TPS发送时交易数被平均到三个池里）

| 自变量          | 参数                                                         |
| --------------- | ------------------------------------------------------------ |
| 网络拓扑        | 2 peer（不同组织）、3 orderer（raft）                        |
| 交易类型        | set、get                                                     |
| 发送TPS         | 100 - 3000 （各次TPS测试时长10s）                            |
| orderer共识配置 | 交易最多等待时间1s、单笔交易最大大小99MB、单个区块最多交易数300、单个区块所有交易最大总大小512k（双节点背书） |

#### PC-P3Chain

- PC-P3Chain自对比：单池单合约 vs 3池3合约（各池各1个合约，各个合约的具体函数都是set和get两种）

| 自变量   | 参数                              |
| -------- | --------------------------------- |
| 网络拓扑 | 4 dper（并发PBFT）                |
| 交易类型 | set、get                          |
| 发送TPS  | 100 - 4000 （各次TPS测试时长10s） |
| 共识配置 | 定时200ms打包，无提前执行交易     |



### 实验结果

#### PC-Fabric

<img src=".\img\basicTPSvsLatency.png" alt="fabric-tps" style="zoom:200%;" />

为什么曲线图如此趋势？由于在发送TPS从100到800过程中打出一个块的时间逐渐减少，所以时延下降。但达到较大TPS后，由于系统不能再更多处理多余的交易就导致平均等待时间变长。

按Fabric原打包策略，大约凑够128笔交易就打包一个块，那TPS=100时要每隔1.28s才打包一个块，当设定了1s出块，所以打包带来的延时0.5s，加上在peer节点的背书和验证差不多在0.03-0.1波动，所以就有了0.57s的平均时延。TPS=200时，0.64s打包一个块，平均0.32s时延，加上背书和验证，差不多0.4s。当TPS=800时只用0.16s就打包，0.08s时延，加上背书和验证差不多就在0.15s。

相对而言，**PC-Fabric one pool**比**Fabric**的最大TPS有所下降但不超过3%，**PC-Fabric one pool**的平均时延长了0.01-0.02。**PC-Fabric three pool**比**Fabric**的最大TPS下降约5%，在时延上，由于交易被均分到3个池子里，等待达到交易打包交易数的时间就延长了，所以平均时延变久。除此之外，由于并行产生3个区块，虽然fabric上区块能够并行进行raft共识，但是还是将最低时延翻了一倍不到。



**优化**：在数千TPS的应用场景下，Orderer并未达到瓶颈，但是交易的失效率却很高。基于降低失效率的目的，同时保证原来TPS和时延的性能不变，可以去尝试调整Orderer参数，使得在相同TPS情况下，时延也不变。目前相同TPS下，时延翻倍了。可调整的参数有区块的打包



#### PC-P3Chain（粗略）

<img src=".\img\PC-P3Chain_BasicTPSvsLatency.png" alt="PC-P3Chain_BasicTPSvsLatency" style="zoom:200%;" />

| 对照组                            | 内容                                                         |
| --------------------------------- | ------------------------------------------------------------ |
| 单池单合约（1）                   | 测试最佳情况200ms进行打包。只安装一类合约DEMO1对应pool1，发送TPS都是调用DEMO1 |
| 三池三合约（2）                   | 500ms。开了三个池pool1、pool2、pool3，三个池分别对应合约DEMO1、DEMO2、DEMO3，发送TPS轮流调用DEMO1、DEMO2、DEMO3。假如TPS=300，每秒DEMO1-3各100笔 |
| Mid-Process的单池单合约（3）（4） | 测了200ms（3）和800ms（4）的情况。                           |
| Mid-Process的三池三合约（5）      | 200ms测不出来，只有800ms。                                   |

100-2000TPS部分，时延（1）最佳，（2）最差，为什么（2）最差可能是三个池的三个区块最终在异步层（原来上层）串行执行，而MidProc可以并行执行（各池的交易互斥），因此相较（5）还要差些。单池和三池的时延差别在于多区块共识、执行的开销，即使能够并发进行也还是会有额外开销。





## 功能测试

### 实验设置

#### PC-Fabric

##### ~~无效性交易~~

| 测试设计       | 通过FILTER模块在共识之初进行交易无效性检查，排除黑名单之内的交易或只允许白名单之内的交易，提升交易有效率甚至TPS |
| -------------- | ------------------------------------------------------------ |
| 网络拓扑       | 2 peer、3orderer（raft）                                     |
| 交易类型和场景 | 单池合约cc1的set、get函数，只许白名单内成员进行set，不许黑名单内成员get。（上链的get和不上链get的重要区别在于，上链get过程记录在案）（类似于BGP的前缀宣告和检查，宣告只做set，检查只做get，只允许某些BGP做宣告，不允许某些恶意BGP做检查，当然私下检查当然可以。那为什么要弄上链的检查呢？通过宣告和检测的交易，我可以推理出网络的拓扑结构，而某些明确是恶意的BGP再做无中生有的检查时就会导致拓扑出错） |
| 测试内容       | TPS=800（接近最大TPS）下，随机对键为k的数据进行set或get，set k的值固定为v以保障交易不会因为冲突而失效。随机选择白名单之外的节点做set，随机选择黑名单之内的节点做get，这些节点指背书节点。 |
| 模块设计       | FILTER内针对交易的发起者进行黑白名单验证，首先会维持一张黑名单表和白名单表，假设名单是提前内嵌好的，当然想动态修改也是可以设计的，比如FILTER会从某个文件去读黑白名单，或者就是有一个专门的合约来更新黑白名单表。 |

> 不能用FILTER做冲突交易的过滤，交易是否冲突和FILTER后续的SELECT和ORDER等模块都有很大关系，不能提前知道是否冲突。

##### 冲突型交易

| 测试设计       | 对于区块内交易，利用ORDER将read-only型交易提前（甚至是配合SELECT将两批交易内的read-only型交易提取出来打在一个区块，其余的在下一个区块），提升交易有效率。 |
| -------------- | :----------------------------------------------------------- |
| 网络拓扑       | 2 peer、3orderer（raft）                                     |
| 交易类型和场景 | 单池合约cc1的set、get函数                                    |
| 测试内容       | TPS=500/1000/1500/2000（TPS稳定情况下）下，依次对键为k1到k100/k1000/k5000间的数据进行set或get（对每个k进行1次set和3/9次get），set k的值为0/1交替变化。对某个k的set和get交易顺序随机。 |
| 模块设计       | 在ORDER模块内，将所有get类型的交易排在set交易前面。          |
| 设计的考虑     | 由于不断地发出交易，如果只是对某个key不停读写，且每次写入的值不一样，那在持续性地发送交易过程中，只有第一个块的get交易会因为select到块前头而有效，其他的get因为读到的key一直在变化而无论怎么排都无效。另外对于key的范围也要讲究，太窄会出现这种情况，一个还没上链的块内对某个key进行了一次set和一次get交易，但紧接着的一个块又对key进行set和get，这样一来下一个块的get就无效了。（set不会无效） |

##### 依赖型交易

| 测试设计       | 存在ABC三类交易，C的上链条件为AB先上链。通过SELECT和ORDER模块，提升C的上链成功率 |
| -------------- | ------------------------------------------------------------ |
| 网络拓扑       | 2 peer、3orderer（raft）                                     |
| 交易类型和场景 | 单池合约cc1的set函数，只有set An=1和set Bn=1两笔交易存在时，才会触发set Cn=1上链 |
| 测试内容       | TPS=500/1000/1500/2000（TPS稳定情况下）下，不断轮流发送Tx-An、Tx-Bn、Tx-Cn，n属于正整数。 |
| 模块设计       | 设置SELECT只有当交易池里同时存在Tx-An、Tx-Bn、Tx-Cn三笔交易时，才一块将交易取出，并使用ORDER模块将三笔交易按需排列打包。 |
| 设计缘由       | 存在这种情况，假如甲乙双方对一份合同达成共识，各自发起A和B交易来表明自己赞同，而公证机构发起C来检验甲乙确实发起了AB并触发进一步的操作。在没有SELECT和ORDER的设计时，公证机构可能需要不断去检测链上是否有AB，才发起C，而现在就可以直接发出去就不管了，等AB出现了就自动触发。如果需要设置C的超时无效，即甲乙双方没有在一定时间内表明赞同，也是可以设计的。 |


##### 对照组

原Fabric，对标单池的PC-Fabric，无特殊的可编程功能。



#### PC-P3Chain

##### 依赖型交易

| 测试设计       | 存在ABC三类交易，C的上链条件为AB先上链。通过SELECT和ORDER模块，提升C的上链成功率 |
| -------------- | ------------------------------------------------------------ |
| 网络拓扑       | 4 dper（并发PBFT）                                           |
| 交易类型和场景 | 单池合约DEMO1的set函数，只有set An=1和set Bn=1两笔交易存在时，才会触发set Cn=1上链 |
| 测试内容       | TPS=500/1000/1500/2000（TPS稳定情况下）下，不断轮流发送Tx-An、Tx-Bn、Tx-Cn，n属于正整数。（先后到达dper顺序会乱） |
| 模块设计       | 设置SELECT只有当交易池里同时存在Tx-An、Tx-Bn、Tx-Cn三笔交易时，才一块将交易取出（顺序也排好了） |

##### 冲突型交易

| 测试设计       | 存在多笔交易（Set、Get两类）对某个key值进行读写操作（MID-PROCESS阶段）。通过MID-PROCESS模块设计，先后共识的多个区块基于同一个沙盒而非各个独立，降低冲突导致的交易无效率（这是由于前一区块还没上链但后续区块就也紧接着共识，会导致执行时都是基于相同的世界状态，但是实际上前一区块可能会改动状态） |
| -------------- | ------------------------------------------------------------ |
| 网络拓扑       | 4 dper（并发PBFT）                                           |
| 交易类型和场景 | DEMO1的Get和Set                                              |
| 测试内容       | TPS=500/1000/1500/2000（TPS稳定情况下）下，以次对键为k1到k100/k1000/k5000间的数据进行set或get（对每个k进行1次set和3/9次get），set k的值为0/1交替变化。对某个k的set和get交易顺序随机。 |
| 模块设计       | 设计基于Block和Pool的沙盒，基于Block的沙盒，每个区块Mid-Process时就清空沙盒，而Pool的沙盒则不会，多个区块基于状态连续变化的沙盒环境。Pool的沙盒只会在多节点间完全同步的世界状态（交易执行基于的版本控制比较难设计，所以只采用了较简单的方法，多了一个Mid-Process交易执行基于的世界状态）更新后清空，而这个同步的世界状态更新时不会有区块在共识，所以就不会因为上述原因造成交易失效。 |

##### 互斥型交易

测试设计：提升正交交易的并行执行能力。通过SORT实现交易池间的交易都正交



##### 极差型交易

测试设计：存在两类交易，A：例如转账，单笔交易资源占用少、执行块，对交易确认时间要求高；B：例如股票交易，单笔交易存在较长时间（因为等待or计算复杂）。在交易内混杂着AB两类交易时，不能一次性混着将AB打包一块，导致A的确认时延被拉长。通过START和SELECT来降低A的交易确认时延。



### 实验结果

#### PC-Fabric

##### 冲突型交易

<img src=".\img\conflictTxs.png" alt="conflictTxs" style="zoom:200%;" />

（1）Fabric Get:Set=3:1

（2）Fabric Get:Set=9:1

（3）PC-Fabric Get:Set=3:1

（4）PC-Fabric Get:Set=9:1

（1）（2）原始Fabric存在较大冲突，（3）（4）在排序上将Get提前确保尽可能多的Get交易都生效（Get和Set都在同一个区块的情况下），但是当Get跑到Set的后一个区块里时，交易就失效了。

由于Get/Set顺序随机，因此Get交易中50%会无效，因此（1）总体无效率在37.5%上下波动（3/4/2），（2）则是45%（9/10/2）。Get/Set比例较高情况下，由于先后顺序随机，因此Get在Set前的交易数目会较多，因此（2）的失败率高于（1）。（3）（4）的相对较小，不过要确定一个大概的值有点困难，一般情况只有靠近出块时，关于这时入池的某个key的Get和Set交易容易被划分入前后两个区块。



##### 依赖型交易

![dependentTxs](.\img\dependentTxs.png)

理想情况下是Fabric时延为PC-Fabric的两倍（500TPS和1000TPS时基本满足），但由于实际上Fabric中Set C前需要不断访问A和B是否已经Set，所以不断对peer进行本地Get A和B，消耗了资源（0.1s间隔访问），所以最大TPS点提前，没有像原本Fabric的TPS=1500和TPS=2000时的性能。

（500TPS时，Fabric和PC-Fabric基础性能时延0.2s）



#### PC-P3Chain

##### 冲突型交易（还需要多测几遍找到更贴合理论的数据）

![PC-P3Chain_ConflictTxs](.\img\PC-P3Chain_ConflictTxs.png)

（1）BLOCK Get:Set=3:1

（2）BLOCK Get:Set=9:1

（3）POOL Get:Set=3:1

（4）POOL Get:Set=9:1

（1）（2）BLOCK冲突情况是由于Get交易跑到Set交易所在区块的后续区块里了（同区块内顺序不影响有效率），并且后续区块在执行Mid-Process时Set所在区块更改的状态还没更新到链上。以及还有一种同时存在的情况，假如TPS=500，共识上链需要1.3s，当k=100，Get:Set=3:1时，每发送400笔交易，k的值就会颠倒一次（set k=0/1），即只要过0.8s就会颠倒一次，所以在1.3s里Get失效的数目在增加。这种同时存在的情况有个规律，k越小越容易造成失效率增加。另外Get:Set越大，这种颠倒情况带来的失效率就越小。

上图的数据里k=1000的情况都比较符合这个规律，k=100时TPS=1500和TPS=2000的数据怀疑是（1）测的有问题，还有k=5000也是。概率性，测试次数太少没能更贴合上去。

（3）（4）基本不会失效，在POOL情况下存在失效的原因是由于Mid-Process时按顺序执行的两个并发的区块（前一个区块含Get X，后一个区块含Set X），在后续提交到异步共识时顺序被颠倒，但这种情况发生概率很小很小。当然这也在（1）（2）会存在

##### 依赖型交易

![PC-P3Chain_DependentTxs](.\img\PC-P3Chain_DependentTxs.png)

（1）PC-P3Chain commonSelector

（2）PC-P3Chain depSelector

理想下（2）是（1）两倍，不过由于（1）中还要不断本地查询A和B（0.3s间隔），所以随着上链的TPS提高时延逐渐增加（TPS越高也就越多本地查询），最大上链TPS点被提前。
